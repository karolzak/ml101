{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b8cf1d",
   "metadata": {},
   "source": [
    "# Data Science & Machine Learning 101\n",
    "## A 60-Minute Hands-On Learning Session\n",
    "\n",
    "Welcome to this practical introduction to data science and machine learning! In the next hour, we'll walk through a complete ML workflow using the famous Titanic dataset.\n",
    "\n",
    "### What we'll cover:\n",
    "1. **Data Loading & Understanding** (10 min)\n",
    "2. **Exploratory Data Analysis (EDA)** (15 min)\n",
    "3. **Data Preprocessing & Splitting** (10 min)\n",
    "4. **Training ML Models** (15 min)\n",
    "5. **Model Evaluation & Conclusions** (10 min)\n",
    "\n",
    "### Business Context:\n",
    "We'll predict passenger survival on the Titanic - a classic ML problem that demonstrates how data science can uncover patterns and make predictions from historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8562b9d",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Understanding (10 minutes)\n",
    "\n",
    "First, let's import our essential libraries and load the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c907717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfc00a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset from seaborn (built-in dataset)\n",
    "df = sns.load_dataset('titanic')\n",
    "# First look at our data\n",
    "print(\"🔍 DATASET OVERVIEW\")\n",
    "print(f\"Shape: {df.shape} (rows, columns)\")\n",
    "print(\"\\n📋 First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc5411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show basic stats\n",
    "print(f\"🎯 Survival rate: {df['survived'].mean():.1%}\")\n",
    "# min, max, average, median age\n",
    "print(f\"📉 Min age: {df['age'].min():.1f} years\")\n",
    "print(f\"📈 Max age: {df['age'].max():.1f} years\")\n",
    "print(f\"📊 Median age: {df['age'].median():.1f} years\")\n",
    "print(f\"👶 Average age: {df['age'].mean():.1f} years\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dea874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and basic info\n",
    "print(\"📊 DATA TYPES & INFO:\")\n",
    "print(df.info())\n",
    "print(\"\\n📈 BASIC STATISTICS:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85171f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"❓ MISSING VALUES:\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_pct = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Percentage': missing_pct\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19a45d",
   "metadata": {},
   "source": [
    "### 💡 Key Insights from Data Overview:\n",
    "- We have 891 passengers with 15 features\n",
    "- Survival rate was 38.4% (tragic but gives us a balanced prediction problem)\n",
    "- Missing data in 'age' (20%), 'embarked' (0.2%), and 'deck' (77%)\n",
    "- Mix of numerical (age, fare) and categorical (sex, class) variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8aeac4",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA) (15 minutes)\n",
    "\n",
    "Let's explore the data to understand what factors influenced survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f16e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Survival count\n",
    "survival_counts = df['survived'].value_counts()\n",
    "axes[0].bar(['Died', 'Survived'], survival_counts.values, color=['red', 'green'], alpha=0.7)\n",
    "axes[0].set_title('Passenger Survival Distribution')\n",
    "axes[0].set_ylabel('Number of Passengers')\n",
    "\n",
    "# Survival percentage\n",
    "survival_pct = df['survived'].value_counts(normalize=True) * 100\n",
    "axes[1].pie(survival_pct.values, labels=['Died', 'Survived'], autopct='%1.1f%%', \n",
    "           colors=['red', 'green'])\n",
    "axes[1].set_title('Survival Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"📊 Total passengers: {len(df)}\")\n",
    "print(f\"🟢 Survived: {df['survived'].sum()} ({df['survived'].mean():.1%})\")\n",
    "print(f\"🔴 Died: {(df['survived'] == 0).sum()} ({(df['survived'] == 0).mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1bbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze survival by passenger class and gender\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Survival by class\n",
    "survival_by_class = df.groupby('class')['survived'].mean().sort_values(ascending=False)\n",
    "survival_by_class.plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Survival Rate by Passenger Class')\n",
    "axes[0].set_ylabel('Survival Rate')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Survival by gender\n",
    "survival_by_sex = df.groupby('sex')['survived'].mean().sort_values(ascending=False)\n",
    "survival_by_sex.plot(kind='bar', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Survival Rate by Gender')\n",
    "axes[1].set_ylabel('Survival Rate')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 SURVIVAL RATES:\")\n",
    "print(\"By Class:\")\n",
    "for class_name, rate in survival_by_class.items():\n",
    "    print(f\"  {class_name}: {rate:.1%}\")\n",
    "print(\"By Gender:\")\n",
    "for gender, rate in survival_by_sex.items():\n",
    "    print(f\"  {gender}: {rate:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age and fare analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Age distribution by survival\n",
    "df.boxplot(column='age', by='survived', ax=axes[0,0])\n",
    "axes[0,0].set_title('Age Distribution by Survival')\n",
    "axes[0,0].set_xlabel('Survived (0=Died, 1=Survived)')\n",
    "\n",
    "# Fare distribution by survival\n",
    "df.boxplot(column='fare', by='survived', ax=axes[0,1])\n",
    "axes[0,1].set_title('Fare Distribution by Survival')\n",
    "axes[0,1].set_xlabel('Survived (0=Died, 1=Survived)')\n",
    "\n",
    "# Age histogram by survival\n",
    "for survived in [0, 1]:\n",
    "    subset = df[df['survived'] == survived]['age'].dropna()\n",
    "    axes[1,0].hist(subset, alpha=0.6, label=f'Survived: {bool(survived)}', bins=20)\n",
    "axes[1,0].set_xlabel('Age')\n",
    "axes[1,0].set_ylabel('Count')\n",
    "axes[1,0].set_title('Age Distribution by Survival')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Survival by embarkation port\n",
    "survival_by_port = df.groupby('embarked')['survived'].mean()\n",
    "survival_by_port.plot(kind='bar', ax=axes[1,1], color='lightgreen')\n",
    "axes[1,1].set_title('Survival Rate by Embarkation Port')\n",
    "axes[1,1].set_ylabel('Survival Rate')\n",
    "axes[1,1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0154d2a",
   "metadata": {},
   "source": [
    "### 📊 How to Read the Age & Fare Distribution Graphs\n",
    "\n",
    "**Top Left - Age Box Plot:**\n",
    "- The box shows the middle 50% of ages (25th to 75th percentile)\n",
    "- The line inside the box = median age\n",
    "- The \"whiskers\" (lines extending from box) show the range of typical values\n",
    "- Dots above/below whiskers = outliers (unusually young/old passengers)\n",
    "- **Key insight**: Survivors had a slightly younger median age than non-survivors\n",
    "\n",
    "**Top Right - Fare Box Plot:**\n",
    "- Similar to age, but for ticket prices\n",
    "- Survivors clearly paid higher fares on average\n",
    "- Many high-fare outliers among survivors (expensive first-class tickets)\n",
    "- **Key insight**: Higher fare = better accommodations = better survival chances\n",
    "\n",
    "**Bottom Left - Age Histogram:**\n",
    "- Pink bars = passengers who died\n",
    "- Brown/gold bars = passengers who survived  \n",
    "- Height of bars = number of passengers in that age group\n",
    "- **Key insight**: Children (0-15) and young adults had better survival rates\n",
    "\n",
    "**Bottom Right - Embarkation Port Bar Chart:**\n",
    "- C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "- Height = survival rate for passengers boarding at each port\n",
    "- **Key insight**: Cherbourg passengers had highest survival rate (~55%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d097a20b",
   "metadata": {},
   "source": [
    "### 🔍 What These Patterns Tell Us:\n",
    "\n",
    "**Business Insights from the Graphs:**\n",
    "\n",
    "1. **Socioeconomic Factor**: The fare distribution clearly shows wealth mattered for survival\n",
    "   - First-class passengers (high fares) had better access to lifeboats\n",
    "   - Third-class passengers were often trapped below deck\n",
    "\n",
    "2. **Age Advantage**: The age histogram reveals the \"women and children first\" protocol\n",
    "   - Children under 15 had disproportionately high survival rates\n",
    "   - Working-age adults (20-40) had mixed outcomes\n",
    "\n",
    "3. **Port of Embarkation**: Cherbourg passengers had better survival rates because:\n",
    "   - Cherbourg was a premium port with more first-class passengers\n",
    "   - These passengers boarded later and had better cabin locations\n",
    "\n",
    "**How to Spot Patterns:**\n",
    "- Look for **shifts** in box plot medians (center lines)\n",
    "- Compare **spreads** of the boxes (variability within groups)\n",
    "- Notice **overlapping vs. separate** distributions\n",
    "- Count **outliers** - they often tell interesting stories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined analysis: Class and Gender\n",
    "survival_pivot = df.groupby(['class', 'sex'])['survived'].mean().unstack()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "survival_pivot.plot(kind='bar', width=0.8)\n",
    "plt.title('Survival Rate by Class and Gender')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.legend(title='Gender')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 SURVIVAL RATES BY CLASS AND GENDER:\")\n",
    "print(survival_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd9a47",
   "metadata": {},
   "source": [
    "### 💡 Key EDA Insights:\n",
    "1. **\"Women and Children First\"**: Females had 74% survival rate vs 19% for males\n",
    "2. **Class Matters**: First class had 63% survival vs 24% in third class  \n",
    "3. **Age Factor**: Children had higher survival rates\n",
    "4. **Fare**: Higher fares (better accommodations) correlated with survival\n",
    "5. **Embarkation**: Passengers from Cherbourg had higher survival rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a21386c",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing & Splitting (10 minutes)\n",
    "\n",
    "Now let's prepare our data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061af849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e321e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "print(\"🔧 PREPROCESSING DATA FOR MACHINE LEARNING\")\n",
    "\n",
    "# Handle missing values first\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Fill missing ages with median age by gender and class\n",
    "df_clean['age'] = df_clean.groupby(['sex', 'class'])['age'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "# Fill missing embarked with most common port (Southampton)\n",
    "df_clean['embarked'] = df_clean['embarked'].fillna(df_clean['embarked'].mode().values[0])\n",
    "df_clean['embark_town'] = df_clean['embark_town'].fillna(df_clean['embark_town'].mode().values[0])\n",
    "\n",
    "# Drop columns with too many missing values or not useful for prediction\n",
    "df_clean = df_clean.drop(['deck', 'alive'], axis=1)\n",
    "\n",
    "print(f\"✅ Missing values handled\")\n",
    "print(f\"✅ Dataset shape after cleaning: {df_clean.shape}\")\n",
    "\n",
    "# Check remaining missing values\n",
    "remaining_missing = df_clean.isnull().sum()\n",
    "print(f\"✅ Remaining missing values: {remaining_missing.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a62e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7929cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and engineer features\n",
    "feature_columns = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
    "\n",
    "# Create new features (feature engineering)\n",
    "df_clean['family_size'] = df_clean['sibsp'] + df_clean['parch'] + 1\n",
    "df_clean['is_alone'] = (df_clean['family_size'] == 1).astype(int)\n",
    "df_clean['age_group'] = pd.cut(df_clean['age'], bins=[0, 12, 18, 35, 60, 100], \n",
    "                               labels=['Child', 'Teen', 'Adult', 'Middle-aged', 'Senior'])\n",
    "\n",
    "# Add these new features to our feature list\n",
    "feature_columns.extend(['family_size', 'is_alone', 'age_group'])\n",
    "\n",
    "X = df_clean[feature_columns].copy()\n",
    "y = df_clean['survived']\n",
    "\n",
    "print(f\"✅ Features selected: {len(feature_columns)}\")\n",
    "print(f\"✅ New features created: family_size, is_alone, age_group\")\n",
    "print(f\"✅ Target variable: survived ({y.sum()} survivors)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ce738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab4788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle categorical variables using Label Encoding\n",
    "X_processed = X.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "categorical_features = ['sex', 'embarked', 'age_group']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X_processed[feature] = le.fit_transform(X_processed[feature])\n",
    "    label_encoders[feature] = le\n",
    "    print(f\"✅ Encoded {feature}: {le.classes_}\")\n",
    "\n",
    "print(f\"\\n🔍 Processed features shape: {X_processed.shape}\")\n",
    "X_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, \n",
    "    test_size=0.2,      # 20% for testing\n",
    "    random_state=42,    # for reproducibility\n",
    "    stratify=y          # maintain same survival ratio in train/test\n",
    ")\n",
    "\n",
    "print(\"📊 DATA SPLIT SUMMARY:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(df_clean):.1%})\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(df_clean):.1%})\")\n",
    "print(f\"Training survival rate: {y_train.mean():.1%}\")\n",
    "print(f\"Test survival rate: {y_test.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc666d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['age', 'fare', 'family_size']\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test_scaled[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "print(\"✅ Features scaled using StandardScaler\")\n",
    "print(\"📊 Scaling example (fare):\") \n",
    "print(f\"  Original range: ${X_train['fare'].min():.2f} - ${X_train['fare'].max():.2f}\")\n",
    "print(f\"  Scaled range: {X_train_scaled['fare'].min():.2f} - {X_train_scaled['fare'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4792b30",
   "metadata": {},
   "source": [
    "### 💡 Preprocessing Summary:\n",
    "- **Features**: 10 predictive features selected (including 3 engineered features)\n",
    "- **Encoding**: Categorical variables converted to numbers\n",
    "- **Splitting**: 80% training, 20% testing\n",
    "- **Scaling**: Numerical features standardized (mean=0, std=1)\n",
    "\n",
    "Data is now ready for machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d0681b",
   "metadata": {},
   "source": [
    "## 4. Training ML Models (15 minutes)\n",
    "\n",
    "Let's train two different types of models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ca74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Logistic Regression (Linear model, good baseline)\n",
    "print(\"🤖 TRAINING MODEL 1: LOGISTIC REGRESSION\")\n",
    "print(\"📝 Good for: interpretability, baseline performance, linear relationships\")\n",
    "\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "lr_train_pred = lr_model.predict(X_train_scaled)\n",
    "lr_test_pred = lr_model.predict(X_test_scaled)\n",
    "lr_test_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate accuracies\n",
    "lr_train_accuracy = accuracy_score(y_train, lr_train_pred)\n",
    "lr_test_accuracy = accuracy_score(y_test, lr_test_pred)\n",
    "\n",
    "print(f\"✅ Training accuracy: {lr_train_accuracy:.3f}\")\n",
    "print(f\"✅ Test accuracy: {lr_test_accuracy:.3f}\")\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "feature_importance_lr = pd.DataFrame({\n",
    "    'feature': X_processed.columns,\n",
    "    'importance': abs(lr_model.coef_[0])\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n🔍 TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "for idx, row in feature_importance_lr.head().iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7bbca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Random Forest (Ensemble model, handles non-linear relationships)\n",
    "print(\"🌳 TRAINING MODEL 2: RANDOM FOREST\")\n",
    "print(\"📝 Good for: non-linear relationships, feature interactions, robust predictions\")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,    # number of trees\n",
    "    random_state=42,\n",
    "    max_depth=5          # prevent overfitting\n",
    ")\n",
    "rf_model.fit(X_train, y_train)  # Random Forest doesn't require scaling\n",
    "\n",
    "# Make predictions\n",
    "rf_train_pred = rf_model.predict(X_train)\n",
    "rf_test_pred = rf_model.predict(X_test)\n",
    "rf_test_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate accuracies\n",
    "rf_train_accuracy = accuracy_score(y_train, rf_train_pred)\n",
    "rf_test_accuracy = accuracy_score(y_test, rf_test_pred)\n",
    "\n",
    "print(f\"✅ Training accuracy: {rf_train_accuracy:.3f}\")\n",
    "print(f\"✅ Test accuracy: {rf_test_accuracy:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': X_processed.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n🔍 TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "for idx, row in feature_importance_rf.head().iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models side by side\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest'],\n",
    "    'Training Accuracy': [lr_train_accuracy, rf_train_accuracy],\n",
    "    'Test Accuracy': [lr_test_accuracy, rf_test_accuracy],\n",
    "    'Overfitting': [lr_train_accuracy - lr_test_accuracy, rf_train_accuracy - rf_test_accuracy]\n",
    "})\n",
    "\n",
    "print(\"📊 MODEL COMPARISON:\")\n",
    "print(models_comparison)\n",
    "\n",
    "# Visualize feature importance comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Logistic Regression feature importance\n",
    "feature_importance_lr.head(8).plot(x='feature', y='importance', kind='barh', ax=axes[0])\n",
    "axes[0].set_title('Logistic Regression - Feature Importance')\n",
    "axes[0].set_xlabel('Absolute Coefficient Value')\n",
    "\n",
    "# Random Forest feature importance  \n",
    "feature_importance_rf.head(8).plot(x='feature', y='importance', kind='barh', ax=axes[1])\n",
    "axes[1].set_title('Random Forest - Feature Importance')\n",
    "axes[1].set_xlabel('Importance Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8b2f1",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation & Conclusions (10 minutes)\n",
    "\n",
    "Let's evaluate our models thoroughly and draw business conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d242644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_proba, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"📈 {model_name.upper()} EVALUATION:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.3f} (Overall correct predictions)\")\n",
    "    print(f\"  Precision: {precision:.3f} (Of predicted survivors, how many actually survived)\")\n",
    "    print(f\"  Recall:    {recall:.3f} (Of actual survivors, how many we correctly identified)\")\n",
    "    print(f\"  F1-Score:  {f1:.3f} (Balanced measure of precision and recall)\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluate both models\n",
    "lr_metrics = evaluate_model(y_test, lr_test_pred, lr_test_proba, \"Logistic Regression\")\n",
    "print()\n",
    "rf_metrics = evaluate_model(y_test, rf_test_pred, rf_test_proba, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Logistic Regression confusion matrix\n",
    "cm_lr = confusion_matrix(y_test, lr_test_pred)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Logistic Regression - Confusion Matrix')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "# Random Forest confusion matrix\n",
    "cm_rf = confusion_matrix(y_test, rf_test_pred)\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title('Random Forest - Confusion Matrix')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 CONFUSION MATRIX INTERPRETATION:\")\n",
    "print(\"  Top-left: True Negatives (correctly predicted deaths)\")\n",
    "print(\"  Top-right: False Positives (incorrectly predicted survivors)\")\n",
    "print(\"  Bottom-left: False Negatives (missed survivors)\")\n",
    "print(\"  Bottom-right: True Positives (correctly predicted survivors)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe29d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model predictions on sample passengers\n",
    "print(\"🔍 SAMPLE PREDICTIONS:\")\n",
    "print(\"Let's see how our models predict on some test passengers:\\n\")\n",
    "\n",
    "sample_indices = [0, 5, 10, 15, 20]\n",
    "for i in sample_indices:\n",
    "    actual = y_test.iloc[i]\n",
    "    lr_pred = lr_test_pred[i]\n",
    "    rf_pred = rf_test_pred[i]\n",
    "    lr_prob = lr_test_proba[i]\n",
    "    rf_prob = rf_test_proba[i]\n",
    "    \n",
    "    # Get original passenger info\n",
    "    passenger_idx = y_test.index[i]\n",
    "    passenger_info = df_clean.loc[passenger_idx]\n",
    "    \n",
    "    print(f\"Passenger {i+1}:\")\n",
    "    print(f\"  Class: {passenger_info['class']}, Gender: {passenger_info['sex']}, Age: {passenger_info['age']:.0f}\")\n",
    "    print(f\"  Actual: {'Survived' if actual else 'Died'}\")\n",
    "    print(f\"  LR Prediction: {'Survived' if lr_pred else 'Died'} (confidence: {lr_prob:.2f})\")\n",
    "    print(f\"  RF Prediction: {'Survived' if rf_pred else 'Died'} (confidence: {rf_prob:.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388bf817",
   "metadata": {},
   "source": [
    "## 📊 Business Conclusions & Insights\n",
    "\n",
    "### Key Findings:\n",
    "1. **Gender was the strongest predictor**: Women had ~74% survival rate vs ~19% for men\n",
    "2. **Passenger class mattered significantly**: First class passengers had much higher survival rates\n",
    "3. **Age played a role**: Children had better survival chances\n",
    "4. **Family connections**: Being alone vs. having family affected survival differently\n",
    "\n",
    "### Model Performance:\n",
    "- **Random Forest** slightly outperformed Logistic Regression\n",
    "- Both models achieved ~80% accuracy on test data\n",
    "- Models successfully learned the \"women and children first\" principle\n",
    "\n",
    "### Business Applications:\n",
    "1. **Historical Analysis**: Understanding social dynamics and emergency protocols\n",
    "2. **Risk Assessment**: Identifying factors that influence survival in emergencies  \n",
    "3. **Policy Making**: Informing safety regulations and evacuation procedures\n",
    "4. **Feature Engineering**: Demonstrated how creating new features (family_size, is_alone) can improve predictions\n",
    "\n",
    "### Next Steps:\n",
    "- Try more advanced models (XGBoost, Neural Networks)\n",
    "- Collect more features (cabin location, exact boarding time)\n",
    "- Cross-validation for more robust model evaluation\n",
    "- Deploy model as a web service for interactive predictions\n",
    "\n",
    "**🎯 Mission Accomplished!** In 60 minutes, we've completed a full machine learning workflow from data exploration to actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73cb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc7405a5",
   "metadata": {},
   "source": [
    "# Bonus Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7001cb1",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Here's a simple, visualizable Python example of gradient descent using NumPy for a toy problem: minimizing a basic quadratic function like:\n",
    "\n",
    "𝑓(𝑤)=(𝑤−3)^2\n",
    " \n",
    "which has its minimum at 𝑤=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to minimize and its derivative (gradient)\n",
    "def loss(w):\n",
    "    # Simple quadratic loss function: (w - 3)^2\n",
    "    # In python we use ** for exponentiation\n",
    "    return (w - 3)**2\n",
    "\n",
    "def gradient(w):\n",
    "    # derivative of (w - 3)^2 is 2 * (w - 3)\n",
    "    return 2 * (w - 3)\n",
    "\n",
    "# Gradient Descent parameters\n",
    "w = 0.0                 # initial guess\n",
    "learning_rate = 0.1     # step size\n",
    "epochs = 25             # number of iterations\n",
    "\n",
    "# To store history for plotting\n",
    "w_history = [w]\n",
    "loss_history = [loss(w)]\n",
    "\n",
    "# Gradient Descent loop\n",
    "for i in range(epochs):\n",
    "    grad = gradient(w)\n",
    "    w = w - learning_rate * grad\n",
    "    w_history.append(w)\n",
    "    loss_history.append(loss(w))\n",
    "    print(f\"Epoch {i+1:02d}: w = {w:.4f}, loss = {loss(w):.6f}\")\n",
    "    if i>5 and len(set([round(l, 6) for l in loss_history[-5:]]))==1:\n",
    "        print(\"Convergence reached, stopping early.\")\n",
    "        break  \n",
    "\n",
    "# Plotting the descent\n",
    "w_vals = np.linspace(-1, 7, 100)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(w_vals, loss(w_vals), label='Loss function')\n",
    "plt.scatter(w_history, loss_history, color='red', label='Gradient descent steps')\n",
    "plt.plot(w_history, loss_history, color='red', linestyle='--')\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Gradient Descent on (w - 3)^2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbb358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Loss function and its gradient\n",
    "def loss(w1, w2):\n",
    "    return (w1 - 3)**2 + (w2 + 2)**2\n",
    "\n",
    "def gradient(w1, w2):\n",
    "    grad_w1 = 2 * (w1 - 3)\n",
    "    grad_w2 = 2 * (w2 + 2)\n",
    "    return grad_w1, grad_w2\n",
    "\n",
    "# Initial point\n",
    "w1, w2 = 0, 0\n",
    "learning_rate = 0.1\n",
    "epochs = 30\n",
    "\n",
    "# Store history for plotting\n",
    "w1_history, w2_history, loss_history = [w1], [w2], [loss(w1, w2)]\n",
    "\n",
    "for i in range(epochs):\n",
    "    dw1, dw2 = gradient(w1, w2)\n",
    "    w1 -= learning_rate * dw1\n",
    "    w2 -= learning_rate * dw2\n",
    "    w1_history.append(w1)\n",
    "    w2_history.append(w2)\n",
    "    loss_history.append(loss(w1, w2))\n",
    "    print(f\"Epoch {i+1:02d}: w1 = {w1:.4f}, w2 = {w2:.4f}, loss = {loss(w1, w2):.6f}\")\n",
    "\n",
    "# Create grid for surface plot\n",
    "W1, W2 = np.meshgrid(np.linspace(-1, 6, 100), np.linspace(-5, 2, 100))\n",
    "Z = loss(W1, W2)\n",
    "\n",
    "# 3D plot\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(W1, W2, Z, cmap='viridis', alpha=0.7)\n",
    "ax.plot(w1_history, w2_history, loss_history, color='r', marker='o', label='Gradient descent path')\n",
    "ax.set_xlabel('w1')\n",
    "ax.set_ylabel('w2')\n",
    "ax.set_zlabel('Loss')\n",
    "ax.set_title('Gradient Descent in 2D Parameter Space')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55bc6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77ec32d1",
   "metadata": {},
   "source": [
    "### Binary Cross-entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813aa449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prediction probabilities from 0.01 to 0.99\n",
    "y_hat = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# Compute loss for y=1 and y=0\n",
    "loss_y1 = -np.log(y_hat)          # when true label y = 1\n",
    "loss_y0 = -np.log(1 - y_hat)      # when true label y = 0\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(y_hat, loss_y1, label='y=1', color='green')\n",
    "plt.plot(y_hat, loss_y0, label='y=0', color='red')\n",
    "plt.xlabel('Predicted Probability (ŷ)')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Binary Cross-Entropy Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth and model predictions\n",
    "y_true = np.array([1, 0, 1, 1, 0])\n",
    "y_pred = np.array([0.9, 0.1, 0.4, 0.8, 0.2])\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "def binary_cross_entropy(y, y_hat):\n",
    "    # Clip to avoid log(0)\n",
    "    y_hat = np.clip(y_hat, 1e-15, 1 - 1e-15)\n",
    "    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "# Per-sample loss\n",
    "print(\"Per-sample loss:\")\n",
    "per_sample_loss = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "# print y_true, y_pred, per_sample_loss one under another and make sure they are aligned\n",
    "for true, pred, loss in zip(y_true, y_pred, per_sample_loss):\n",
    "    print(f\"True: {true}, Pred: {pred:.2f}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Overall cross-entropy loss\n",
    "print(\"\\nOverall cross-entropy loss:\")\n",
    "loss = binary_cross_entropy(y_true, y_pred)\n",
    "print(f\"Cross-entropy loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3ce9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
