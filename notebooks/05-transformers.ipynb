{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253e2cda",
   "metadata": {},
   "source": [
    "# ðŸ¤– Transformers & Attention Mechanism\n",
    "\n",
    "**Master the architecture that powers modern AI**\n",
    "\n",
    "In this notebook, you'll build deep intuition for how attention works and why transformers have revolutionized AI.  \n",
    "We'll progress from simple examples to using state-of-the-art models like BERT, GPT, and T5.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Why attention matters** - See how it solves the fundamental limitations of RNNs\n",
    "2. **How attention works** - Understand the Query-Key-Value mechanism with clear visuals\n",
    "3. **Multi-head attention** - Learn why multiple attention heads capture richer patterns\n",
    "4. **Transformer architecture** - Connect the pieces into encoder and decoder blocks\n",
    "5. **Apply pretrained models** - Use transformers for real tasks with Hugging Face\n",
    "\n",
    "**Prerequisites:** Familiarity with embeddings (Notebook 02) and basic PyTorch (Notebook 03)\n",
    "\n",
    "**Duration:** 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e898d",
   "metadata": {},
   "source": [
    "# 0. Setup & Motivation\n",
    "\n",
    "### Why Do We Need Transformers?\n",
    "\n",
    "Consider this sentence:\n",
    "\n",
    "> *\"The animal didn't cross the street because **it** was too tired.\"*\n",
    "\n",
    "What does **\"it\"** refer to?  \n",
    "The animal, of course!  \n",
    "But for a neural network to understand this, it needs to connect \"it\" (position 7) with \"animal\" (position 1) - that's a long-range dependency.  \n",
    "Take a look at an example below and see how RNN (sequential words processing) vs Transformers (attention/parallel processing) looks:\n",
    "\n",
    "![](./assets/rnn_vs_transformers.png)\n",
    "\n",
    "**The RNN Problem:**\n",
    "- RNNs process sequences one step at a time: wordâ‚ â†’ wordâ‚‚ â†’ wordâ‚ƒ â†’ ...\n",
    "- Information from wordâ‚ must pass through ALL intermediate steps to reach wordâ‚‡\n",
    "- This creates a **bottleneck**: early information gets compressed and lost\n",
    "\n",
    "**The Transformer Solution:**\n",
    "- Every word can directly \"look at\" every other word in parallel\n",
    "- Wordâ‚‡ can directly attend to wordâ‚ without going through steps 2-6\n",
    "- Much faster and better at capturing long-range dependencies\n",
    "\n",
    "Let's see how this revolutionized AI! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Transformers library\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "\n",
    "# Add helpers to path\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from helpers.transformers import (\n",
    "    plot_evolution_timeline, \n",
    "    plot_rnn_vs_transformer,\n",
    "    create_causal_mask,\n",
    "    visualize_causal_masking,\n",
    "    visualize_attention_heads\n",
    ")\n",
    "\n",
    "# Configure environment\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set numpy print options to prevent line wrapping\n",
    "np.set_printoptions(linewidth=200, suppress=True)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "torch.set_printoptions(linewidth=200, sci_mode=False)\n",
    "\n",
    "print(f\"âœ… Setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85070319",
   "metadata": {},
   "source": [
    "### The Evolution of Sequence Models\n",
    "\n",
    "Let's visualize how we got from RNNs to modern transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the evolution of sequence models\n",
    "plot_evolution_timeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d2210",
   "metadata": {},
   "source": [
    "### Understanding the Information Bottleneck\n",
    "\n",
    "When RNNs process sequences, they maintain a **hidden state** - a fixed-size vector that tries to capture all the important information seen so far. This creates a fundamental problem:\n",
    "\n",
    "**The Bottleneck Problem:**\n",
    "- As the sequence gets longer, the hidden state must compress more and more information\n",
    "- Early information gets repeatedly transformed and can be lost or distorted\n",
    "- The network must decide what to \"remember\" and what to \"forget\" at each step\n",
    "- This is like trying to summarize a whole book into a single paragraph - crucial details get lost!\n",
    "\n",
    "**Example:** In \"The animal didn't cross the street because it was too tired\"\n",
    "- By the time we reach \"it\", the RNN has processed 7 words\n",
    "- Information about \"animal\" has passed through 6 transformations\n",
    "- Each transformation risks losing or distorting that information\n",
    "- The longer the distance, the harder it is to maintain the connection\n",
    "\n",
    "**Transformers solve this** by allowing direct connections between any two words, no matter how far apart they are. Let's visualize this difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6390964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RNN sequential processing vs Transformer parallel attention\n",
    "plot_rnn_vs_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662c623f",
   "metadata": {},
   "source": [
    "### Why This Matters in Practice\n",
    "\n",
    "The ability to process sequences in parallel and maintain direct connections has revolutionary implications:\n",
    "\n",
    "**1. Training Speed**\n",
    "- RNNs: Must process word-by-word â†’ Can't parallelize â†’ Slow training\n",
    "- Transformers: Process all words at once â†’ Highly parallelizable â†’ Fast training on GPUs\n",
    "\n",
    "**2. Long-Range Dependencies**\n",
    "- RNNs: Struggle with dependencies >10-20 words apart\n",
    "- Transformers: Can handle dependencies across 100s or 1000s of words\n",
    "\n",
    "**3. Scalability**\n",
    "- RNNs: Hit diminishing returns as you make them bigger\n",
    "- Transformers: Scale beautifully to billions of parameters (GPT-3 has 175B!)\n",
    "\n",
    "This is why transformers have become the foundation for:\n",
    "- ðŸ’¬ **Language Models**: ChatGPT, Claude, Gemini\n",
    "- ðŸ” **Search**: Google BERT for understanding queries\n",
    "- ðŸŒ **Translation**: Modern translation systems\n",
    "- ðŸ“ **Summarization**: Automatic document summarization\n",
    "- ðŸ’» **Code Generation**: GitHub Copilot, CodeLlama\n",
    "- ðŸ–¼ï¸ **Vision**: Even image models (Vision Transformers)\n",
    "\n",
    "Now let's dive into **how** attention actually works! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38affe33",
   "metadata": {},
   "source": [
    "# 1. Attention Intuition\n",
    "\n",
    "Now that we understand *why* transformers are powerful, let's dive into *how* they work. At the heart of transformers is the **attention mechanism**.\n",
    "\n",
    "### The Database Search Analogy\n",
    "\n",
    "Think of attention like searching a database:\n",
    "\n",
    "**Imagine you're in a library looking for information about \"neural networks\":**\n",
    "\n",
    "1. ðŸ” **Query**: Your search request - \"I want books about neural networks\"\n",
    "2. ðŸ”‘ **Keys**: Book titles and descriptions - what each book offers\n",
    "3. ðŸ“š **Values**: The actual book content - the information you'll retrieve\n",
    "\n",
    "**How it works:**\n",
    "- You compare your **query** against each book's **key** (title/description)\n",
    "- Books with titles/descriptions matching \"neural networks\" get high relevance scores\n",
    "- You retrieve a weighted combination of **values** (content) based on these scores\n",
    "- Books highly relevant to your query contribute more to your final understanding\n",
    "\n",
    "### Attention Does the Same Thing!\n",
    "\n",
    "In transformers, each word uses this same mechanism:\n",
    "\n",
    "- **Query (Q)**: \"What am I looking for?\" - Each word asks a question\n",
    "- **Key (K)**: \"What do I offer?\" - Each word advertises its content  \n",
    "- **Value (V)**: \"Here's my information\" - The actual information to retrieve\n",
    "\n",
    "**Example:** In \"The cat sat on the mat\"\n",
    "- When processing \"sat\", its **query** might be: \"Who is performing this action?\"\n",
    "- The word \"cat\" has a **key** that says: \"I'm a noun, the subject\"\n",
    "- If the similarity is high, \"sat\" pays attention to \"cat\"\n",
    "- \"sat\" then retrieves \"cat\"'s **value** (semantic meaning) to build its understanding\n",
    "\n",
    "Let's see this with a concrete toy example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 3-word attention example: \"cat likes fish\"\n",
    "words = [\"cat\", \"likes\", \"fish\"]\n",
    "\n",
    "# Step 0: Start with initial embeddings (lookup table approach)\n",
    "# In real transformers, these come from an embedding layer (nn.Embedding)\n",
    "# Each word has its own unique learned embedding vector in a lookup table\n",
    "\n",
    "# Create a lookup table: word -> embedding vector\n",
    "embedding_table = {\n",
    "    \"cat\": np.array([1.0, 0.5, 0.2, 0.8]),\n",
    "    \"likes\": np.array([0.6, 0.9, 0.7, 0.3]),\n",
    "    \"fish\": np.array([0.4, 0.3, 0.8, 0.9])\n",
    "}\n",
    "\n",
    "# Look up embeddings for our words\n",
    "embeddings = np.array([embedding_table[word] for word in words])\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print(\"\\nInitial Embeddings (from embedding layer):\")\n",
    "print(embeddings)\n",
    "print(\"Shape:\", embeddings.shape, \"- (num_words, embedding_dim)\")\n",
    "\n",
    "# Step 1: Define learned projection matrices W_Q, W_K, W_V\n",
    "# These are learned during training and transform embeddings into Q, K, V\n",
    "# In real transformers, these are nn.Linear layers\n",
    "\n",
    "# W_Q projects embeddings to \"what am I looking for?\"\n",
    "W_Q = np.array([\n",
    "    [0.1, 0.2, 0.9, 0.1],\n",
    "    [0.2, 0.1, 0.5, 0.8],\n",
    "    [0.8, 0.5, 0.1, 0.1],\n",
    "    [0.1, 0.8, 0.2, 0.9]\n",
    "])\n",
    "\n",
    "# W_K projects embeddings to \"what do I offer?\"\n",
    "W_K = np.array([\n",
    "    [0.2, 0.1, 0.1, 0.2],\n",
    "    [0.1, 0.2, 0.9, 0.1],\n",
    "    [0.1, 0.1, 0.2, 0.8],\n",
    "    [0.2, 0.8, 0.1, 0.1]\n",
    "])\n",
    "\n",
    "# W_V projects embeddings to \"what information do I contain?\"\n",
    "W_V = np.array([\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "# Step 2: Compute Q, K, V through linear projections\n",
    "# This is the key insight: Q, K, V all come from the SAME embeddings\n",
    "# but are transformed differently\n",
    "Q = embeddings @ W_Q  # Query = Embeddings Ã— W_Q\n",
    "K = embeddings @ W_K  # Key = Embeddings Ã— W_K\n",
    "V = embeddings @ W_V  # Value = Embeddings Ã— W_V\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LINEAR PROJECTIONS (the key insight!):\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nQuery matrix W_Q (learned during training):\")\n",
    "print(W_Q)\n",
    "print(\"\\nQ = Embeddings @ W_Q:\")\n",
    "print(Q)\n",
    "print(\"â†’ 'cat' query:\", Q[0], \"(looking for what?)\")\n",
    "print(\"â†’ 'likes' query:\", Q[1], \"(looking for what?)\")\n",
    "print(\"â†’ 'fish' query:\", Q[2], \"(looking for what?)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nKey matrix W_K (learned during training):\")\n",
    "print(W_K)\n",
    "print(\"\\nK = Embeddings @ W_K:\")\n",
    "print(K)\n",
    "print(\"â†’ 'cat' key:\", K[0], \"(offers what?)\")\n",
    "print(\"â†’ 'likes' key:\", K[1], \"(offers what?)\")\n",
    "print(\"â†’ 'fish' key:\", K[2], \"(offers what?)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nValue matrix W_V (learned during training):\")\n",
    "print(W_V)\n",
    "print(\"\\nV = Embeddings @ W_V:\")\n",
    "print(V)\n",
    "print(\"â†’ 'cat' value:\", V[0], \"(what information?)\")\n",
    "print(\"â†’ 'likes' value:\", V[1], \"(what information?)\")\n",
    "print(\"â†’ 'fish' value:\", V[2], \"(what information?)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ’¡ KEY INSIGHT:\")\n",
    "print(\"=\"*60)\n",
    "print(\"âœ“ All three (Q, K, V) start from the SAME embeddings\")\n",
    "print(\"âœ“ Different linear projections (W_Q, W_K, W_V) create different roles\")\n",
    "print(\"âœ“ In PyTorch: Q = nn.Linear(d_model, d_model)(embeddings)\")\n",
    "print(\"âœ“ These projection matrices are LEARNED during training!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca784fa",
   "metadata": {},
   "source": [
    "### Step 1: Compute Similarity Scores\n",
    "\n",
    "Now we compute how well each query matches each key using **dot product**:\n",
    "- High dot product = high similarity = \"these words should pay attention to each other\"\n",
    "- Low dot product = low similarity = \"these words are less relevant to each other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ccf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute similarity scores (Q @ K^T)\n",
    "# This creates a 3x3 matrix: each query vs each key\n",
    "similarity_scores = Q @ K.T\n",
    "\n",
    "print(\"Similarity Scores (before scaling):\")\n",
    "print(\"Rows = queries (who's looking), Columns = keys (who's being looked at)\\n\")\n",
    "print(\"        cat    likes  fish\")\n",
    "for i, word in enumerate(words):\n",
    "    print(f\"{word:6s} {similarity_scores[i]}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Notice:\")\n",
    "print(\"   - 'likes' has high scores with both 'cat' and 'fish' (it relates them)\")\n",
    "print(\"   - 'cat' and 'fish' pay attention to 'likes' (the action connecting them)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2874a8b",
   "metadata": {},
   "source": [
    "### Step 2: Scale the Scores\n",
    "\n",
    "We divide by $\\sqrt{d_k}$ where $d_k$ is the dimension of our vectors (4 in this case).\n",
    "\n",
    "**Why?** When dimensions get large, dot products can become very large, causing gradients to vanish during training. Scaling keeps the values in a reasonable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6253acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scale by sqrt(d_k)\n",
    "d_k = Q.shape[1]  # dimension = 4\n",
    "scaled_scores = similarity_scores / np.sqrt(d_k)\n",
    "\n",
    "print(f\"Scaling factor: âˆš{d_k} = {np.sqrt(d_k):.2f}\")\n",
    "print(\"\\nScaled Scores:\")\n",
    "print(\"        cat     likes   fish\")\n",
    "for i, word in enumerate(words):\n",
    "    print(f\"{word:6s} {scaled_scores[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b637335",
   "metadata": {},
   "source": [
    "### Step 3: Apply Softmax to Get Attention Weights\n",
    "\n",
    "Softmax converts the scores into **probabilities** that sum to 1. This tells us: \"For each word, how much attention should it pay to each other word?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da9eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply softmax to get attention weights\n",
    "# Softmax converts scores to probabilities (each row sums to 1)\n",
    "attention_weights = np.exp(scaled_scores) / np.exp(scaled_scores).sum(axis=1, keepdims=True)\n",
    "\n",
    "print(\"Attention Weights (probabilities):\")\n",
    "print(\"Each row sums to 1.0 - shows how each word distributes its attention\\n\")\n",
    "print(\"       cat    likes  fish  | sum\")\n",
    "for i, word in enumerate(words):\n",
    "    row_sum = attention_weights[i].sum()\n",
    "    print(f\"{word:6s} {attention_weights[i][0]:.3f}  {attention_weights[i][1]:.3f}  {attention_weights[i][2]:.3f} | {row_sum:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Interpretation:\")\n",
    "print(\"   - 'cat' pays 33% attention to itself, 51% to 'likes', 16% to 'fish'\")\n",
    "print(\"   - 'likes' distributes attention across all three words fairly evenly\")\n",
    "print(\"   - 'fish' focuses most on 'likes' (the verb connecting it to the subject)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49065b78",
   "metadata": {},
   "source": [
    "### Step 4: Compute Weighted Sum of Values\n",
    "\n",
    "Finally, we use the attention weights to compute a weighted average of the **values**. Each word's output is a mixture of all words' information, weighted by attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d53891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compute weighted sum of values\n",
    "output = attention_weights @ V\n",
    "\n",
    "print(\"Output (weighted combination of values):\\n\")\n",
    "for i, word in enumerate(words):\n",
    "    print(f\"{word:6s}: {output[i]}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight:\")\n",
    "print(\"   Each word's output is now a rich representation that includes\")\n",
    "print(\"   information from OTHER words, weighted by relevance!\")\n",
    "print(\"   - 'cat' output includes info from 'likes' (what action it performs)\")\n",
    "print(\"   - 'likes' output includes info from both 'cat' and 'fish' (subject & object)\")\n",
    "print(\"   - 'fish' output includes info from 'likes' (how it relates to the sentence)\")\n",
    "print(\"\\n   This is how transformers build contextual understanding! ðŸŽ¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8939c044",
   "metadata": {},
   "source": [
    "### Visualizing Attention Weights\n",
    "\n",
    "Let's create a heatmap to see the attention pattern visually. Brighter colors = stronger attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19939a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights as a heatmap\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(\n",
    "    attention_weights, \n",
    "    annot=True, \n",
    "    fmt='.3f',\n",
    "    xticklabels=words,\n",
    "    yticklabels=words,\n",
    "    cmap='YlOrRd',\n",
    "    cbar_kws={'label': 'Attention Weight'},\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    square=True,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Keys (tokens being attended to)', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Queries (tokens doing the attending)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Attention Pattern: \"cat likes fish\"', fontsize=13, fontweight='bold', pad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š How to read this heatmap:\")\n",
    "print(\"   - Each ROW shows: 'Where does this word look?'\")\n",
    "print(\"   - Each COLUMN shows: 'Who is looking at this word?'\")\n",
    "print(\"   - Brighter colors = stronger attention\")\n",
    "print(\"\\n   Example: Row 2 ('likes') has fairly even attention across all words,\")\n",
    "print(\"            meaning 'likes' gathers information from the entire sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6d4d5",
   "metadata": {},
   "source": [
    "### The Big Picture: What Did We Just Do?\n",
    "\n",
    "Let's summarize the **attention mechanism** we just walked through:\n",
    "\n",
    "```markdown\n",
    "Input Embeddings\n",
    "      â†“\n",
    "   â”Œâ”€â”€â”´â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”\n",
    "   â†“     â†“      â†“      \n",
    "   W_q   W_k    W_v    â† Three SEPARATE learned matrices\n",
    "   â†“     â†“      â†“\n",
    "   Q     K      V      â† Three INDEPENDENT vectors\n",
    "   â””â”€â”€â”¬â”€â”€â”˜      â”‚\n",
    "      â†“         â”‚\n",
    "   Q @ K.T      â”‚      â† Q and K compute SCORES\n",
    "      â†“         â”‚\n",
    "   Softmax      â”‚\n",
    "      â†“         â”‚\n",
    "   Weights â”€â”€â”€â”€â”€â”˜      â† Weights are applied to V\n",
    "      â†“\n",
    "   Output\n",
    "```\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "1. **Dynamic Attention**: Unlike fixed patterns, the model learns what to pay attention to based on the input\n",
    "2. **Contextual Understanding**: Each word's output incorporates information from relevant words\n",
    "3. **Differentiable**: All operations are differentiable, so we can train with backpropagation\n",
    "4. **Parallelizable**: All attention computations happen simultaneously (not sequential like RNNs)\n",
    "\n",
    "**What's Next?**\n",
    "\n",
    "This was attention for a tiny 3-word example. In real transformers:\n",
    "- Embeddings are 512-1024 dimensions (not just 4!)\n",
    "- Sequences can be hundreds or thousands of tokens long\n",
    "- We use **multiple attention heads** to capture different types of relationships\n",
    "- We add **positional encodings** so the model knows word order\n",
    "\n",
    "Let's see the formal mathematical definition next! ðŸŽ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19900155",
   "metadata": {},
   "source": [
    "## 2. The Attention Formula\n",
    "\n",
    "Now let's formalize what we learned into the canonical **scaled dot-product attention** function that powers all transformer models.\n",
    "\n",
    "### The Mathematical Definition\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- **Q** (Query): Matrix of shape `(seq_len, d_k)` - \"What am I looking for?\"\n",
    "- **K** (Key): Matrix of shape `(seq_len, d_k)` - \"What do I offer?\"\n",
    "- **V** (Value): Matrix of shape `(seq_len, d_v)` - \"Here's my information\"\n",
    "- **d_k**: Dimension of queries and keys (used for scaling)\n",
    "\n",
    "### Breaking Down the Formula (Step by Step)\n",
    "\n",
    "This formula performs 4 operations in sequence:\n",
    "\n",
    "**Step 1: Compute Similarity Matrix** â†’ $QK^T$\n",
    "- Matrix multiply queries with transposed keys\n",
    "- Result shape: `(seq_len, seq_len)` - attention score for every pair of positions\n",
    "- Example: Position 3 attending to position 1 gets score at `[3, 1]`\n",
    "   \n",
    "**Step 2: Scale the Scores** â†’ $\\frac{QK^T}{\\sqrt{d_k}}$\n",
    "- Divide by square root of key dimension\n",
    "- **Why?** When d_k is large (e.g., 512), dot products can become very large\n",
    "- Large values â†’ extreme softmax outputs (all weight on one position) â†’ vanishing gradients\n",
    "- Scaling keeps values in a reasonable range for stable training\n",
    "   \n",
    "**Step 3: Apply Softmax** â†’ $\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$\n",
    "- Convert scores to probabilities: each row sums to 1.0\n",
    "- Now each row represents: \"How should this position distribute its attention?\"\n",
    "- High scores become high probabilities, low scores become near-zero\n",
    "   \n",
    "**Step 4: Weighted Sum of Values** â†’ $\\text{softmax}(...)V$\n",
    "- Multiply attention weights by values matrix\n",
    "- Each output vector is a **weighted mixture** of all value vectors\n",
    "- Result shape: `(seq_len, d_v)` - contextualized representation for each position\n",
    "\n",
    "Let's implement this as a reusable PyTorch function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929b9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor of shape (batch_size, seq_len_q, d_k)\n",
    "        K: Key tensor of shape (batch_size, seq_len_k, d_k)\n",
    "        V: Value tensor of shape (batch_size, seq_len_v, d_v)\n",
    "        mask: Optional mask tensor of shape (batch_size, seq_len_q, seq_len_k)\n",
    "              where 0 indicates positions that should be masked out\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output of shape (batch_size, seq_len_q, d_v)\n",
    "        attention_weights: Attention weights of shape (batch_size, seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    # Get dimension for scaling\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Step 1: Compute attention scores (Q @ K^T)\n",
    "    # Shape: (batch_size, seq_len_q, seq_len_k)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided (set masked positions to -inf before softmax)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Step 3: Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 4: Compute weighted sum of values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "print(\"âœ… Scaled dot-product attention function defined!\")\n",
    "print(\"\\nThis is the EXACT attention mechanism used in:\")\n",
    "print(\"  - BERT, GPT, T5, LLaMA, Claude, ChatGPT, and every transformer model!\")\n",
    "print(\"  - The only difference is the dimensionality and how Q, K, V are computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a05d1",
   "metadata": {},
   "source": [
    "### Verify Attention Properties\n",
    "\n",
    "Let's verify that our attention weights have the correct properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da0cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example input: batch of 1, sequence length 5, embedding dimension 8\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "\n",
    "# ðŸ“ Where do embeddings come from in real transformers?\n",
    "# \n",
    "# Input Pipeline:\n",
    "# 1. Tokenization: \"The cat sat\" â†’ [101, 2543, 2068, 102] (token IDs)\n",
    "# 2. Embedding Lookup: Each token ID â†’ Dense vector from learned embedding matrix\n",
    "#    - Embedding matrix shape: (vocab_size, d_model) e.g., (30000, 768)\n",
    "#    - Each token gets its own learned vector representation\n",
    "# 3. Positional Encoding: Add position information (we'll cover in Section 4!)\n",
    "# 4. Linear Projections: Embeddings â†’ Q, K, V via learned weight matrices W_Q, W_K, W_V\n",
    "#\n",
    "# For this demo, we use random embeddings to focus purely on the attention mechanism\n",
    "torch.manual_seed(SEED)\n",
    "embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# In self-attention, Q = K = V all come from the same source\n",
    "# Real transformers compute: Q = XW_Q, K = XW_K, V = XW_V (learned linear transformations)\n",
    "Q = embeddings\n",
    "K = embeddings\n",
    "V = embeddings\n",
    "\n",
    "print(f\"Input shape: {embeddings.shape}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Sequence length: {seq_len}\")\n",
    "print(f\"  Embedding dimension: {d_model}\")\n",
    "\n",
    "# Apply attention\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "\n",
    "print(f\"\\nâœ… Shapes are correct!\")\n",
    "print(f\"   Input:  (batch={batch_size}, seq_len={seq_len}, d_model={d_model})\")\n",
    "print(f\"   Output: (batch={batch_size}, seq_len={seq_len}, d_model={d_model})\")\n",
    "print(f\"   Weights: (batch={batch_size}, seq_len={seq_len}, seq_len={seq_len})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe35a8d",
   "metadata": {},
   "source": [
    "### Testing Our Attention Function\n",
    "\n",
    "Let's test our implementation with a real example using PyTorch tensors. \n",
    "\n",
    "**Key concept:** Even with random embeddings, attention produces valid probability distributions and maintains the correct tensor dimensions throughout the computation.\n",
    "\n",
    "**What we're testing:**\n",
    "\n",
    "- Create a batch of embeddings (simulating a 5-word sentence)- Check that attention weights have the right mathematical properties\n",
    "\n",
    "- Apply our attention function to see what patterns emerge- Verify that the output shapes are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property 1: Each row should sum to 1 (probability distribution)\n",
    "row_sums = attn_weights.sum(dim=-1)\n",
    "print(\"Attention weights - row sums (should all be 1.0):\")\n",
    "print(row_sums)\n",
    "print(f\"âœ“ All rows sum to 1: {torch.allclose(row_sums, torch.ones_like(row_sums))}\")\n",
    "\n",
    "# Property 2: All weights should be non-negative (from softmax)\n",
    "print(f\"\\nâœ“ All weights non-negative: {(attn_weights >= 0).all().item()}\")\n",
    "\n",
    "# Property 3: Weights should be between 0 and 1\n",
    "print(f\"âœ“ All weights in [0,1]: {((attn_weights >= 0) & (attn_weights <= 1)).all().item()}\")\n",
    "\n",
    "# Visualize the attention pattern\n",
    "print(\"\\nðŸ“Š Attention weights matrix:\")\n",
    "print(\"   (showing how much each position attends to every other position)\\n\")\n",
    "attn_matrix = attn_weights[0].detach().numpy()  # Remove batch dimension\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "sns.heatmap(\n",
    "    attn_matrix,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='viridis',\n",
    "    square=True,\n",
    "    cbar_kws={'label': 'Attention Weight'},\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_xlabel('Key Position', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Query Position', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Self-Attention Pattern (Random Embeddings)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b38b1fa",
   "metadata": {},
   "source": [
    "### Understanding Masking: Controlling What the Model Can \"See\"\n",
    "\n",
    "In many scenarios, we need to **prevent attention to certain positions**.  \n",
    "Masking is how we control which tokens can attend to which other tokens.  \n",
    "\n",
    "**Two Main Types of Masking:**\n",
    "\n",
    "1. **Padding Mask** (for variable-length sequences)\n",
    "  - Problem: Batches contain sentences of different lengths â†’ pad shorter ones with special `<PAD>` tokens\n",
    "  - Solution: Mask prevents attention to padding positions\n",
    "  - Example: `[\"The cat sat\", \"Hello\"]` â†’ `[\"The cat sat <PAD>\", \"Hello <PAD> <PAD>\"]`\n",
    "  - Mask ensures \"Hello\" doesn't waste attention on meaningless padding tokens\n",
    "\n",
    "2. **Causal Mask** (for autoregressive generation)\n",
    "  - Problem: When generating text, token at position `i` shouldn't see future tokens (positions > `i`)\n",
    "  - Solution: Mask creates a **lower triangular** pattern - position `i` only sees positions â‰¤ `i`\n",
    "  - This is how GPT generates: \"The cat\" â†’ predict \"sat\" â†’ \"The cat sat\" â†’ predict next word\n",
    "  - During training, we can process the whole sequence in parallel while enforcing this constraint!\n",
    "\n",
    "#### How Masking Works Technically:\n",
    "\n",
    "Before applying softmax, we set masked positions to `-inf`:\n",
    "```python\n",
    "scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "```\n",
    "\n",
    "When softmax is applied, `exp(-inf) = 0`, so masked positions get zero attention weight!\n",
    "\n",
    "Let's visualize causal masking in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef875589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a causal mask using the helper function\n",
    "# Position i can only attend to positions <= i (no looking ahead)\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(\"Causal Mask (1 = allowed, 0 = blocked):\")\n",
    "print(causal_mask.numpy().astype(int))\n",
    "\n",
    "# Apply attention with causal mask\n",
    "masked_output, masked_attn = scaled_dot_product_attention(\n",
    "    Q, K, V, \n",
    "    mask=causal_mask.unsqueeze(0)  # Add batch dimension\n",
    ")\n",
    "\n",
    "# Visualize the effect of causal masking\n",
    "visualize_causal_masking(attn_weights, masked_attn, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bc1b62",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "**What we've learned:**\n",
    "\n",
    "1. âœ… **The Attention Formula**: `softmax(QK^T / âˆšd_k)V` is universal across all transformers\n",
    "2. âœ… **Implementation**: Built a working attention function in just ~15 lines of PyTorch\n",
    "3. âœ… **Properties**: Attention weights are probabilities (sum to 1, non-negative)\n",
    "4. âœ… **Masking**: Can control which positions attend to which (crucial for generation)\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "- This SAME function is used in BERT (encoder), GPT (decoder), and T5 (both)\n",
    "- The only differences are:\n",
    "  - How Q, K, V are computed (linear projections we'll see next)\n",
    "  - Whether masking is applied\n",
    "  - How many attention heads are used (multi-head attention - coming up!)\n",
    "\n",
    "**Next up:** Multi-head attention - running multiple attention operations in parallel to capture different types of relationships! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25587a16",
   "metadata": {},
   "source": [
    "# 3. Multi-Head Attention\n",
    "\n",
    "Now that we understand single-head attention, let's explore why transformers use **multiple attention heads** running in parallel.\n",
    "\n",
    "### Why Multiple Heads?\n",
    "\n",
    "Think back to our library analogy. Imagine searching for books about \"neural networks\":\n",
    "\n",
    "**Single Head (One Librarian):**\n",
    "- One person searches using one strategy\n",
    "- Might focus only on exact title matches\n",
    "- Could miss books with relevant content but different titles\n",
    "\n",
    "**Multi-Head (Team of Librarians):**\n",
    "- **Librarian 1**: Searches by exact keyword matches\n",
    "- **Librarian 2**: Searches by topic similarity\n",
    "- **Librarian 3**: Searches by author expertise\n",
    "- **Librarian 4**: Searches by publication date and citations\n",
    "\n",
    "Each librarian uses a different strategy, then they combine their findings for a richer result!\n",
    "\n",
    "### Multi-Head Attention Does the Same!\n",
    "\n",
    "Different attention heads learn to capture different types of relationships:\n",
    "\n",
    "- **Head 1**: Syntactic relationships (subject-verb agreement)\n",
    "- **Head 2**: Coreference (pronouns to their referents)\n",
    "- **Head 3**: Semantic similarity (related concepts)\n",
    "- **Head 4**: Long-range dependencies\n",
    "- ... and more!\n",
    "\n",
    "**The key insight:** The model learns these specializations automatically during training. We don't tell it what each head should do - it figures out what's useful!\n",
    "\n",
    "### How It Works\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "where each head is:\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$\n",
    "\n",
    "**Steps:**\n",
    "1. **Project**: Transform Q, K, V into h different representations (one per head)\n",
    "2. **Compute**: Run attention independently for each head\n",
    "3. **Concatenate**: Combine all head outputs\n",
    "4. **Project**: Transform concatenated output back to original dimension\n",
    "\n",
    "Let's implement this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd285d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism.\n",
    "    \n",
    "    Instead of one attention function, we run h attention operations in parallel,\n",
    "    each with different learned linear projections. This allows the model to\n",
    "    attend to information from different representation subspaces.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V (for ALL heads combined)\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, d_k).\n",
    "        Transpose to shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1. Linear projections\n",
    "        Q = self.W_q(query)  # (batch_size, seq_len, d_model)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # 2. Split into multiple heads\n",
    "        Q = self.split_heads(Q, batch_size)  # (batch_size, num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "        \n",
    "        # 3. Apply attention for each head\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all heads\n",
    "            mask = mask.unsqueeze(1)\n",
    "        \n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        # attn_output shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 4. Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 5. Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Create multi-head attention layer\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Create sample input\n",
    "torch.manual_seed(SEED)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Apply multi-head attention (self-attention: Q=K=V=x)\n",
    "output, attention_weights = mha(x, x, x)\n",
    "\n",
    "print(\"Multi-Head Attention Test:\")\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Number of heads: {num_heads}\")\n",
    "print(f\"  Dimension per head: {d_model // num_heads}\")\n",
    "print(f\"  Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nâœ… Multi-head attention working correctly!\")\n",
    "print(f\"   Each of the {num_heads} heads processes {d_model // num_heads}-dimensional representations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d1d198",
   "metadata": {},
   "source": [
    "### Visualizing Different Attention Heads\n",
    "\n",
    "Let's see what different heads actually learn by using a pretrained BERT model. We'll look at how different heads attend to the same sentence.\n",
    "\n",
    "**Note:** BERT has 12 layers, each with 12 attention heads = 144 heads total! Each learns different patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011cc287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained BERT with attention outputs\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name, output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "# Prepare a sentence\n",
    "sentence = \"The cat sat on the mat because it was comfortable\"\n",
    "print(f\"Analyzing: '{sentence}'\\n\")\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\\n\")\n",
    "\n",
    "# Get model outputs with attention\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    attentions = outputs.attentions  # Tuple of (num_layers, batch, num_heads, seq_len, seq_len)\n",
    "\n",
    "print(f\"BERT Architecture:\")\n",
    "print(f\"  Layers: {len(attentions)}\")\n",
    "print(f\"  Heads per layer: {attentions[0].shape[1]}\")\n",
    "print(f\"  Attention matrix shape per head: {attentions[0].shape[2:4]}\")\n",
    "print(f\"\\n  Total attention heads: {len(attentions) * attentions[0].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc34c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns from different heads in the same layer\n",
    "layer_to_visualize = 5  # Middle layer (0-indexed)\n",
    "heads_to_show = [0, 3, 7, 11]  # Show 4 different heads\n",
    "\n",
    "# Use the function\n",
    "visualize_attention_heads(attentions, layer_to_visualize, heads_to_show, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa3fbd3",
   "metadata": {},
   "source": [
    "### Analyzing Coreference: What does \"it\" attend to?\n",
    "\n",
    "Let's zoom in on the pronoun \"it\" and see which heads connect it to its referent (\"mat\" or \"cat\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05393aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the position of \"it\" in the tokens\n",
    "it_pos = tokens.index('it')\n",
    "print(f\"Position of 'it': {it_pos}\")\n",
    "print(f\"Tokens: {tokens}\\n\")\n",
    "\n",
    "# Analyze attention from \"it\" across all layers and heads\n",
    "# We'll see which words \"it\" pays most attention to\n",
    "\n",
    "print(\"Top 3 words that 'it' attends to (across different layers):\\n\")\n",
    "\n",
    "for layer_idx in [0, 5, 11]:  # Early, middle, late layers\n",
    "    print(f\"Layer {layer_idx + 1}:\")\n",
    "    \n",
    "    # Average attention across all heads in this layer\n",
    "    layer_attn = attentions[layer_idx][0]  # (num_heads, seq_len, seq_len)\n",
    "    avg_attn = layer_attn.mean(dim=0)  # Average over heads: (seq_len, seq_len)\n",
    "    \n",
    "    # Get attention from \"it\" to all other tokens\n",
    "    it_attention = avg_attn[it_pos].detach().numpy()\n",
    "    \n",
    "    # Get top 3 attended tokens (excluding special tokens and itself)\n",
    "    top_indices = np.argsort(it_attention)[::-1]\n",
    "    \n",
    "    count = 0\n",
    "    for idx in top_indices:\n",
    "        token = tokens[idx]\n",
    "        weight = it_attention[idx]\n",
    "        \n",
    "        # Skip special tokens and the word itself\n",
    "        if token not in ['[CLS]', '[SEP]', '[PAD]', 'it'] and count < 3:\n",
    "            print(f\"  {token:12s} â†’ {weight:.3f}\")\n",
    "            count += 1\n",
    "    print()\n",
    "\n",
    "print(\"ðŸ’¡ Observation:\")\n",
    "print(\"   - Early layers often focus on nearby words and syntax\")\n",
    "print(\"   - Middle layers start capturing semantic relationships\")\n",
    "print(\"   - Later layers refine task-specific patterns\")\n",
    "print(\"   - Different layers solve different sub-problems!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee6243",
   "metadata": {},
   "source": [
    "### Key Takeaways: Multi-Head Attention\n",
    "\n",
    "**What we've learned:**\n",
    "\n",
    "1. âœ… **Multiple Perspectives**: Each head learns to focus on different aspects of relationships\n",
    "   - Syntax, semantics, coreference, position, etc.\n",
    "   \n",
    "2. âœ… **Parallel Processing**: All heads compute simultaneously\n",
    "   - Much faster than sequential processing\n",
    "   - Each head works with smaller dimensions (d_model / num_heads)\n",
    "\n",
    "3. âœ… **Automatic Specialization**: We don't specify what each head should learn\n",
    "   - The model figures out useful patterns through training\n",
    "   - Different heads naturally specialize in complementary ways\n",
    "\n",
    "4. âœ… **Richer Representations**: Combining multiple heads captures more information\n",
    "   - Single head: One way to relate words\n",
    "   - Multi-head: Multiple complementary relationships\n",
    "\n",
    "**The Power of Ensemble:**\n",
    "\n",
    "Think of multi-head attention like having multiple experts on a panel:\n",
    "- Each expert (head) has their own specialty\n",
    "- They all analyze the same input independently\n",
    "- Their insights are combined for a richer understanding\n",
    "- No single expert has to be perfect - they complement each other!\n",
    "\n",
    "**Next up:** Understanding positional encoding - how transformers know word order! ðŸ“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f62ef9",
   "metadata": {},
   "source": [
    "# 4. Positional Encoding\n",
    "\n",
    "So far we've learned how attention allows words to \"look at\" each other. But there's a critical problem we haven't addressed yet...\n",
    "\n",
    "### The Problem: Attention is Order-Agnostic!\n",
    "\n",
    "Consider these two sentences:\n",
    "- \"The **cat** chased the **dog**\"\n",
    "- \"The **dog** chased the **cat**\"\n",
    "\n",
    "These have **completely different meanings**, but if we only use attention without position information, the model would treat them identically!\n",
    "\n",
    "**Why?** Attention computes:\n",
    "- Similarity scores between all pairs of words\n",
    "- Weighted combinations based on content\n",
    "\n",
    "But it doesn't inherently know which word came first!\n",
    "\n",
    "Let's demonstrate this with a concrete example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that attention is order-agnostic\n",
    "# Create embeddings for \"cat\" and \"dog\"\n",
    "torch.manual_seed(SEED)\n",
    "cat_embedding = torch.randn(1, 1, 8)\n",
    "dog_embedding = torch.randn(1, 1, 8)\n",
    "\n",
    "# Sentence 1: \"cat dog\" (cat then dog)\n",
    "sentence1 = torch.cat([cat_embedding, dog_embedding], dim=1)\n",
    "\n",
    "# Sentence 2: \"dog cat\" (dog then cat) - SAME EMBEDDINGS, different order\n",
    "sentence2 = torch.cat([dog_embedding, cat_embedding], dim=1)\n",
    "\n",
    "# Set print options for clean display (both numpy and torch)\n",
    "np.set_printoptions(linewidth=200, suppress=True)\n",
    "torch.set_printoptions(linewidth=200, sci_mode=False)\n",
    "\n",
    "print(\"Sentence 1 (cat dog):\")\n",
    "print(sentence1)\n",
    "\n",
    "print(\"\\nSentence 2 (dog cat):\")\n",
    "print(sentence2)\n",
    "\n",
    "# Apply self-attention to both (Q=K=V)\n",
    "_, attn1 = scaled_dot_product_attention(sentence1, sentence1, sentence1)\n",
    "_, attn2 = scaled_dot_product_attention(sentence2, sentence2, sentence2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ATTENTION WEIGHTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nSentence 1 'cat dog' - Attention pattern:\")\n",
    "print(attn1[0].detach().numpy())\n",
    "\n",
    "print(\"\\nSentence 2 'dog cat' - Attention pattern:\")\n",
    "print(attn2[0].detach().numpy())\n",
    "\n",
    "# Check if patterns are the same (just permuted)\n",
    "print(\"\\nðŸ’¡ Notice:\")\n",
    "print(\"   The attention patterns are permutations of each other!\")\n",
    "print(\"   Position 0â†’1 in sentence1 has same weight as position 1â†’0 in sentence2\")\n",
    "print(\"   This is because attention only looks at CONTENT, not POSITION!\")\n",
    "print(\"\\n   Without positional information, these sentences are indistinguishable! ðŸš¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e839e7",
   "metadata": {},
   "source": [
    "### The Solution: Positional Encoding\n",
    "\n",
    "We need to inject position information into our embeddings. The solution is surprisingly simple:\n",
    "\n",
    "**Add a unique \"position vector\" to each word's embedding!**\n",
    "\n",
    "```\n",
    "Final Embedding = Word Embedding + Position Encoding\n",
    "```\n",
    "\n",
    "**How do we create these position vectors?**\n",
    "\n",
    "Most modern models (BERT, GPT) use **learned positional embeddings**:\n",
    "- Just a lookup table: position 0 â†’ learned vector, position 1 â†’ learned vector, etc.\n",
    "- These vectors are learned during training (like word embeddings)\n",
    "- Simple and effective!\n",
    "\n",
    "The original Transformer paper (2017) used **sinusoidal encoding** (sine/cosine functions), which also works but is less common today.\n",
    "\n",
    "Let's see how this fixes our cat/dog problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d99c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple demonstration: add position information to solve the order problem\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Create word embeddings (same as before)\n",
    "cat_embed = torch.randn(1, 8)\n",
    "dog_embed = torch.randn(1, 8)\n",
    "\n",
    "# Create simple position encodings (in practice, these are learned or sinusoidal)\n",
    "# For this demo, we'll use simple random vectors to represent positions\n",
    "position_0 = torch.tensor([[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]])\n",
    "position_1 = torch.tensor([[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]])\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ADDING POSITIONAL ENCODING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“Œ Sentence 1: 'cat dog'\")\n",
    "print(\"-\" * 70)\n",
    "cat_at_pos0 = cat_embed + position_0\n",
    "dog_at_pos1 = dog_embed + position_1\n",
    "\n",
    "print(f\"Cat at position 0 = cat_embedding + position_0\")\n",
    "print(f\"  Result (first 8 dims): {cat_at_pos0[0].numpy()}\")\n",
    "\n",
    "print(f\"\\nDog at position 1 = dog_embedding + position_1\")\n",
    "print(f\"  Result (first 8 dims): {dog_at_pos1[0].numpy()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Œ Sentence 2: 'dog cat' (SWAPPED)\")\n",
    "print(\"-\" * 70)\n",
    "dog_at_pos0 = dog_embed + position_0\n",
    "cat_at_pos1 = cat_embed + position_1\n",
    "\n",
    "print(f\"Dog at position 0 = dog_embedding + position_0\")\n",
    "print(f\"  Result (first 8 dims): {dog_at_pos0[0].numpy()}\")\n",
    "\n",
    "print(f\"\\nCat at position 1 = cat_embedding + position_1\")\n",
    "print(f\"  Result (first 8 dims): {cat_at_pos1[0].numpy()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ’¡ KEY INSIGHT\")\n",
    "print(\"=\"*70)\n",
    "print(\"âœ“ 'cat' at position 0 â‰  'cat' at position 1 (different position encodings)\")\n",
    "print(\"âœ“ 'dog' at position 0 â‰  'dog' at position 1 (different position encodings)\")\n",
    "print(\"âœ“ Now 'cat dog' and 'dog cat' have DIFFERENT representations!\")\n",
    "print(\"âœ“ The model can now distinguish word order! ðŸŽ¯\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096735fa",
   "metadata": {},
   "source": [
    "### How Positional Encoding Works in Practice\n",
    "\n",
    "In real transformer models, positional encoding is added right at the input:\n",
    "\n",
    "```python\n",
    "# In PyTorch (simplified):\n",
    "class TransformerInput(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_seq_len):\n",
    "        # Word embeddings (learned lookup table)\n",
    "        self.word_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Position embeddings (also learned!)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        # Get word embeddings\n",
    "        word_embeds = self.word_embedding(token_ids)\n",
    "        \n",
    "        # Get position embeddings\n",
    "        positions = torch.arange(len(token_ids))\n",
    "        pos_embeds = self.position_embedding(positions)\n",
    "        \n",
    "        # Add them together!\n",
    "        return word_embeds + pos_embeds\n",
    "```\n",
    "\n",
    "That's it! Just **add position vectors to word vectors** before feeding them into the attention layers.\n",
    "\n",
    "### Key Takeaways: Positional Encoding\n",
    "\n",
    "**What we've learned:**\n",
    "\n",
    "1. âœ… **The Problem**: Attention is order-agnostic\n",
    "   - Without position info: \"cat chased dog\" = \"dog chased cat\"\n",
    "   - The model can't distinguish word order by itself\n",
    "\n",
    "2. âœ… **The Solution**: Add position encodings to embeddings\n",
    "   - Most modern models use learned position embeddings (like BERT, GPT)\n",
    "   - Original Transformer used sinusoidal functions (less common now)\n",
    "   - Simple formula: `final_embedding = word_embedding + position_encoding`\n",
    "\n",
    "3. âœ… **Why It Works**:\n",
    "   - Each position gets its own unique vector\n",
    "   - Same word at different positions â†’ different final representations\n",
    "   - Now the model can tell \"cat dog\" from \"dog cat\"!\n",
    "\n",
    "**The Pipeline So Far:**\n",
    "\n",
    "```\n",
    "1. Tokenize: \"The cat sat\" â†’ [101, 2543, 2068]\n",
    "2. Word Embeddings: token IDs â†’ vectors\n",
    "3. Position Encodings: add position info\n",
    "4. Multi-Head Attention: compute relationships\n",
    "5. Output: contextualized representations\n",
    "```\n",
    "\n",
    "**Next up:** Assembling the complete Transformer architecture! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
